\documentclass{article}

\usepackage{amsthm,amsmath,amssymb,indentfirst,float}
\usepackage{verbatim}
\usepackage[sort,longnamesfirst]{natbib}
\newcommand{\pcite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\ncite}[1]{\citeauthor{#1}, \citeyear{#1}}
\DeclareMathOperator{\logit}{logit}

\usepackage{geometry}
%\geometry{hmargin=1.025in,vmargin={1.25in,2.5in},nohead,footskip=0.5in} 
%\geometry{hmargin=1.025in,vmargin={1.25in,0.75in},nohead,footskip=0.5in} 
%\geometry{hmargin=2.5cm,vmargin={2.5cm,2.5cm},nohead,footskip=0.5in}

\renewcommand{\baselinestretch}{1.25}

\usepackage{amsbsy,amsmath,amsthm,amssymb,graphicx}

\setlength{\baselineskip}{0.3in} \setlength{\parskip}{.05in}


\newcommand{\cvgindist}{\overset{\text{d}}{\longrightarrow}}
\DeclareMathOperator{\PR}{Pr} \DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}


\newcommand{\sX}{{\mathsf X}}
\newcommand{\tQ}{\tilde Q}
\newcommand{\cU}{{\cal U}}
\newcommand{\cX}{{\cal X}}
\newcommand{\tbeta}{\tilde{\beta}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\txi}{\tilde{\xi}}




\title{Design Document: Monte Carlo Maximum Likelihood for Two-Stage Hierarchical Models}

\author{Christina Knudson}

\begin{document}
\maketitle{}
\section{Purpose}
The purpose of this package is to perform MCML. It will read in the user's data and the model specifications, simulate random effects, approximate the log likelihood, and use trust to find the parameter estimates that maximize the log likelihood approximation. At the minimum, it will return the point estimates for the parameters along with their standard errors. It will provide confidence intervals. It will be equipped to do a likelihood ratio test in order to do model comparison between nested models.


\section{Notation}
Let $\beta$ be the vector of $p$ fixed effect parameters. Let $X$ be the design matrix for the fixed effects.  Let $U$ be a normal random vector with mean 0, variance matrix $D$, and design matrix $Z$.There are $T$ distinct variance component  $\nu_t,t=1,...,T$.  Denote the density of $U$ by $f_\theta(u)$  with $\theta= (\beta,\nu_{1},...\nu_T)$. Although $f_\theta(u)$ does not actually depend on $\beta$, we write the density like this to keep notation simple.

Let $g$ be the link function with inverse $g^{-1}$. Then $\mu$ is the vector of means such that

\begin{align}
g(\mu) = X \beta + Z U
\end{align}


The choice of the link function is related to the distribution of the data, $\log f_\theta(y|u)$. If the data have a  Bernoulli distribution, the link is $\logit(\mu)$. If the data have a Poisson distribution, the link is $\log (\mu)$. 

For simplicity of future notation, let $\eta=g(\mu)=X \beta + Z U$. Let $c(\eta)$ denote the cumulant function such that the log of the data density can be written as
\begin{align}
Y' \eta - c(\eta) = \sum_i \left[ Y_i \eta_i - c(\eta_i)  \right]
\end{align}

\subsection{D }
For now,  $D$ is assumed  diagonal so that $D= \sum_{t=1}^T \nu_t E_t$, where $\sum E_t = I$.\\

 Later versions of this package will allow other covariance structures. With other structures, $D$ will still be $D= \sum_{t=1}^T \nu_t E_t$, but $E_t$ will no longer be diagonal. We will have a D for distance (such as in the car theft data) and for autoregressive 1.




\section{model fitting function} 
This will be the main command the user will implement. The user will need to specify the response and the predictors using the formula coding similar to the lm command.  They'll need to specify the  family (either binomial or Poisson, though  really any exponential family would work and I could add more later).The user will specify the random effects in the same way as for the aster package. Thus, a sample command with fixed predictors $x_1$ and $x_2$ and with random effects $school$ and $classroom$ (in data set as categorical variables ) would look like
\begin{verbatim}
mcml(y ~ x1+ x2, list(~0+school,~0+classroom),  family.mcml="binomial.mcml", data=schooldat,varcomps.names=c("school","classroom"),varcomps.equal=c(1,2) )
    \end{verbatim}
The family.mcml part will be discussed in section \ref{sec:fam}.   

$\Box$ Most of the time (but not all the time), the random effects formula should be ``0+...'' If the user does not specify ``0+'' then I want to give a warning something like ``Did you mean to start your random effects formula with 0+? Most models require this, so please check if you meant to include it.''

It's possible that the user could want some variance components to be set equal. For example, in the Coull and Agresti flu problem, there are 4 years and random effects for each year. The authors want the within-year variance components to be equal.  There are also variance components for subject-specific intercepts and for the decreased susceptibility to illness in year 4 (since year 4's virus was a repeat). Suppose year is a categorical variable with four levels, and year1 through year4 are dummy variables. Thus, the call could contain these arguments
\begin{verbatim}
mcml(y~year,list(~0+subject,~0+year1,~0+year2,~0+repeet,~0+year3,~0+year4),
 varcomps.equal=c(1,2,2,3,2,2), varcomps.names=c("subject","year","repeet"),
data=flu,family.mcml=bernoulli.mcml)
\end{verbatim}

Thus model.matrix will make Z into a list of 6 design matrices. Since we have 3 distinct variance components, we want 3 design matrices. What we do is we take the design matrices that share a variance component and then cbind them together. Thus we'll have 3 model matrices in our Z list, one for each random effect. We will want to put them in order (1,2,3 according to varcomps.equal) so that the names for each matrix line up ("subject","year","repeet").  Cbinding the design matrices that share a variance component will not effect the way that we compute $\eta$.  Also, the variance estimates should come out in this same order as well.
    
    The next steps: do pql to get a mean for the random effects;  sample the random effects from a standard normal; scale those generated random effects to PQL's mean and variance; implement trust on the objective function; and return estimates, standard errors, and the log likelihood evaluated at those estimates.

The value will contain the point estimates, the Monte Carlo approximation to to the log likelihood, its first and second derivatives. Other things may be added later. We need to have the entire Fisher information matrix; just the standard errors are not good enough. This is because the prediction intervals will need the whole Fisher information matrix.

%%Families%%
\section{Families} \label{sec:fam}
This function will be hidden from the user.  These functions (along with the distribution of random effects) are necessary to approximate the log likelihood.

I'm going to have a class called ``mcml.family'' with a few options (for now, binomial and poisson). Each family  function will output a list including the family name (a character string such as ``binomial''), a function that calculates the value of the cumulant function $c(\eta)$,  a function that calculates the cumulant's first derivative $c'(\eta)$ with the derivative taken with respect to $\eta$, and  a function that calculates the cumulant's second derivative $c''(\eta)$. 

The user will provide the family in the model-fitting function. They can either enter the character string (``binomial.mcml''), the function (binomial.mcml()), or the result of invoking it.  The following code for using the input to determine the family is adapted from glm.

\begin{verbatim}
logDensity<-function(family.mcml)
{
	if(is.character(family.mcml))
		family.mcml<-get(family.mcml,mode="function",envir=parent.frame())
	if(is.function(family.mcml))
		family.mcml<-family.mcml()
	if(!inherits(family.mcml,"mcml.family")) 
		stop(" 'family' not recognized") 
	return(family.mcml)
}
\end{verbatim}
If the user has entered the family as a string, go get the R object with that family name, either from the immediate environment or the parent environment.  If this has happened, ``family`` is now a function.  If ``family'' is a function (either because the user entered it as a function or because of the preceding step), invoke that function.  At this point, ``family.mcml'' should have  class ``mcml.family.'' If this is not the case (maybe because of a typo or maybe because they entered ``poisson'' rather than ``poisson.mcml''), then stop and return an error.


For example, one of the family functions will look like this
\begin{verbatim}
poisson.mcml<-function()
{
	family.mcml<- "poisson.mcml"
	c <- function(eta) exp(eta)
	cp <- function(eta) exp(eta)
	cpp<-function(eta) exp(eta)
	out<-list(family.mcml=family.mcml, c=c,cp=cp,cpp=cpp)
	class(out)<-"mcml.family"
	return(out)
}
\end{verbatim}

Then, to use these functions in order to calculate $c(\eta_i), c'(\eta_i),$ and  $c''(\eta_i)$, I can just call 
\begin{verbatim}
family$c(args)
family$cp(args)
family$cpp(args)
\end{verbatim}

For  binomial, these values (c, cp, and cpp) are:
\begin{align}
c(\eta_i)&=\log(1+e^{\eta_i})\\
c'(\eta_i)&=\dfrac{ e^{{\eta_{i}}}}{ 1+e^{{\eta_{i}}}}\\
c''(\eta_i)&=   \dfrac{e^{{\eta_{i}}}}{  1+ e^{{\eta_{i}}} }  - \dfrac{e^{2{\eta_{i}}}}{    (  1+ e^{{\eta_{i}}})^2}  
\end{align}
and for poisson they are
\begin{align}
c(\eta_i)&=e^{\eta_i}\\
c'(\eta_i)&=e^{{\eta_{i}}}\\
c''(\eta_i)&= e^{{\eta_{i}}}.
\end{align}

Then we use these pieces to create the scalar $c(\eta)$, the vector $c'(\eta)$ and the matrix $c''(\eta)$. We calculate

\begin{align}
c(\eta)= \sum_i c(\eta_i).
\end{align}
 The vector $c'(\eta)$ has components $c'(\eta_i)$. The matrix $c''(\eta)$ is diagonal with diagonal elements $c''(\eta_i)$.

Note: for glm's binomial, the user can choose the link. They can't do that here: we need the link to be logit so that we have an exponential family. I'm going to include the link as one of the things in the family.mcml value just in case they don't already know what the link is.

Also, I'll have a check in here to make sure that the data are valid given the family type. So I'll make sure that if family.mcml is binomial.mcml, the data should be 0's or 1's. If family.mcml is poisson.mcml, then the data should be nonnegative. If that's not true, give an error message.

\section{Log density of the data (el)}

Recall the log of the data density is
\begin{align}
\log f_\theta(y|u) &= Y' \eta +c(\eta) \\
&= \sum_{i} Y_{i} {\eta_{i}} - c({\eta_{i}})
\end{align}
where
\begin{align}
\eta=X\beta+ZU.
\end{align}
The derivative of this with respect to one component, $\eta_j$, is
\begin{align}
\dfrac{\partial}{\partial \eta_j} \log f_\theta(y|u)  = Y_j-c'(\eta_j).
\end{align}
The derivative of the component $\eta_j$ with respect to one of the fixed effect predictors, $\beta_{l_1}$, is
\begin{align}
\dfrac{\partial \eta_j}{\partial \beta_{l_1}} = X_{j{l_1}}
\end{align}

We'd like the derivative of the log of the data density with respect to $\beta$. This can be written using the chain rule as follows:
\begin{align}
\dfrac{\partial}{\partial \beta_{l_1}}  \log f_\theta(y|u) &= \dfrac{\partial \eta_j}{\partial \beta_{l_1}} \dfrac{\partial}{\partial \eta_j} \log f_\theta(y|u) \\
&= \left[ Y_j-c'(\eta_j) \right]  X_{j{l_1}}
\end{align}

The mixed partial derivative (with respect to $\beta_{l_1}$ and $\beta_{l_2}$) of the log data density can be written similarly:
\begin{align}
\dfrac{\partial^2}{\partial \beta_{l_1} \partial \beta_{l_2}}  \log f_\theta(y|u) &=\dfrac{\partial}{\partial \beta_{l_2}} \left( \left[ Y_j-c'(\eta_j) \right]  X_{j{l_1}} \right) \\
&= \dfrac{\partial \eta_j}{\partial \beta_{l_2}} \dfrac{\partial}{\partial \eta_j} \left( \left[ Y_j-c'(\eta_j) \right]  X_{j{l_1}} \right) \\
&= -X_{j{l_1}} X_{j{l_2}} c''(\eta_j) 
 \end{align}


Letting $\nabla c(\eta)$ be a vector with components $c'(\eta_j)$, the first derivative of the log data density can be written in matrix form as:
\begin{align}
\dfrac{\partial}{\partial \beta}  \log f_\theta(y|u) = X' \left[ Y- \nabla c(\eta)  \right].
\end{align}

Letting $\nabla^2 c(\eta)$ be a vector with components $c'(\eta_j)$, the second derivative of the log data density can be written in matrix form as:
\begin{align}
   \frac{\partial^2}{\partial \beta^2} \log f_\theta(y|u) =   X' [ - \nabla^2 c(\eta) ] X
\end{align}


%%getEk
\section{getEk}
Recall $D=\sum \nu_t E_t$. The way we  get different forms of D is by changing $E_t$. We'll have a function that gives the $E_t$, depending on what the form of $D$ is.

\subsection{D diagonal}
When D is diagonal, then $\sum E_t = I$, the identity matrix. The $E_t$ are diagonal with either 1 or 0 on the diagonal (1 if that random effect has $\nu_t$ as a variance component, 0 otherwise).

At this point,  mod.mcml\$z is a list with $T$ design matrices (one for each distinct variance component).  We need to go through and count the  number of columns ($q_t$) for each design matrix $t$ in the list (= the number of random effects based on that variance compnent $\nu_t$).
 Then $\sum q_t$ is the total number of random effects in the model, and $D$ will have that many rows (and columns). Thus, each $E_t$ will have that number of rows and columns as well.  Then we know that $E_1$ has $q_1$ ones on the diagonal, followed by zeros to fill the rest of the diagonal.  $E_2$ has $q_1$ zeros, then $q_2$ ones, then zeros for the rest of the diagonal.

%%distribution of the random effects%%
\section{distribution of the random effects (distRand)}
This will be hidden from the user.  To  approximate the log likelihood and to maximize that approximation, I'm going to need $\log f_\theta(u)$, $\nabla \log f_\theta(u)$, and $\nabla^2 \log f_\theta(u)$.  We will calculate these quantities with these things: $D$ (the variance matrix for the u's), $\nu$ (the length-t vector of variance components), $U$ (the vector of random effects, which I will have generated from the importance sampling), $\mu$ (the mean of the random effects, which is either 0 or $u^*$ from PQL) and the list $z$ (of length T where T is the number of variance components, and each matrix in the list has $q_t$ columns).

%The model.matrix command has turned each the random effects into a model matrix and has outputted the model matrices as a list. 
%%For example, if a random effect is teacher, and the teacher factor has three levels (``Naslund,'' ``Johnson,'' ``Jergens''), then the model matrix would have ``Naslund,'' ``Johnson,'' ``Jergens'' as the three entries in the $3\times1$ matrix. If another random effect is school, where school is a factor with two levels (``South'', ``Central''), then the corresponding model matrix will be $2\times1$. I will need to remember to use ``drop=F'' to keep some matrices from turning into vectors.
%The number of matrices in the list is equal to the number of random effects $T$. 
%%In the example above, we'd get two model matrices, one for each random effect.

Since the random effects are drawn from $N(\mu, D)$, then the distribution of the random effects is:
\begin{align}
\log f_\theta(u) = (-1/2) \log |D| - (1/2) (U-\mu)' D^{-1} (U-\mu)
\end{align}

We will need first and second derivatives with respect to $\nu$. We'll have to take advantage of the form of D.

\subsection{D Diagonal}
In the simplest case, D is diagonal with variance components $\nu_t$s on the diagonal and 0s off diagonal. Remember that, for every $\nu_t$, we can construct a matrix $E_t$ (with dimensions the same as matrix $D$) that has 1s on the diagonal elements corresponding to the elements of D that contain $\nu_t$ and 0s everywhere else.  

We can partition the random effects according to their variance component: $U=(U_1',...,U_T')'$.  Let $D_t$ be the variance matrix for $U_t$ . $D_t$ has $q_t$ rows (and also columns). So $D$ is this:
\begin{align}
D = \begin{bmatrix} D_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & D_T \end{bmatrix}
\end{align}

Since D is diagonal, then $D^{-1}$ is also diagonal. The diagonal entries are now $\dfrac{1}{\nu_1}$,...,$\dfrac{1}{\nu_T}$.

Calculating det(D) is also easy because D is diagonal:
\begin{align}
det(D)= \nu_1^{q_1}...\nu_T^{q_T}
\end{align}

 The log density for $U$ is
\begin{align}
\log f_\theta(u) &= (-1/2) \log |D| - (1/2) (U-\mu)' D^{-1} (U-\mu)\\
&= -\dfrac{1}{2} \left[  \sum_{t=1}^T q_t \log \nu_t   \right]  -\dfrac{1}{2} \sum_{t=1}^T \left[ \dfrac{1}{\nu_t} (U_t-\mu_t)'(U_t-\mu_t)   \right]
\end{align}





%Letting $I_{q_t}$ be the identity matrix with $q_t$ rows (and columns), we can write this as a sum as follows:
%\begin{align}
%\log f_\theta(u)&= \sum_{t=1}^T \log f_\theta(u_t)\\
%&=\sum_{t=1}^T - \dfrac{1}{2} \log |D_t| - \dfrac{1}{2} (U_t-\mu_t)' D_t^{-1} (U_t-\mu_t) \\ 
%&=\sum_{t=1}^T - \dfrac{1}{2} \log |\nu_t^{q_t}| - \dfrac{1}{2 \nu_t} (U_t-\mu_t)' I_{q_t} (U_t-\mu_t) \\ 
%&=\sum_{t=1}^T - \dfrac{q_t}{2} \log \nu_t - \dfrac{1}{2 \nu_t}(U_t-\mu_t) '(U_t-\mu_t)
%\end{align}
Taking the first and second derivative of each summand with respect to its associated $\nu_t$ results in
\begin{align}
\dfrac{\partial}{\partial \nu_t} \log f_\theta(u_t) = - \dfrac{q_t}{2 \nu_t} + \dfrac{1}{2 \nu_t^2}(U_t-\mu_t)'(U_t-\mu_t)
\end{align}
and 
\begin{align}
\dfrac{\partial^2}{\partial \nu_t^2} \log f_\theta(u_t) = \dfrac{q_t}{2 \nu_t^2}- \dfrac{1}{\nu_t^3} (U_t-\mu_t)'(U_t-\mu_t).
\end{align}
Any other derivative is equal to 0. That is, for all $t_1 \neq t_2$,
\begin{align}
\dfrac{\partial}{\partial \nu_{t_1}} \log f_\theta(u_{t_2}) = 0.
\end{align}
Also,
\begin{align}
\dfrac{\partial}{\partial \beta} \log f_\theta(u_{t_2}) = 0.
\end{align}
Thus, if $\nu = (\nu_1,...,\nu_T)$, the gradient of the random effects distribution is
\begin{align}
\dfrac{\partial}{\partial \nu}  \log f_\theta(u) = \begin{bmatrix} - \dfrac{q_1}{2 \nu_1} + \dfrac{1}{2 \nu_1^2} (U_1-\mu_1) ' (U_1-\mu_1) & ... & - \dfrac{q_T}{2 \nu_T} + \dfrac{1}{2 \nu_T^2} (U_T-\mu_T) '(U_T-\mu_T)   \end{bmatrix} 
\end{align}
To calculate this vector, I will create a function to take $q_t$, $U_t$, and $\nu_t$ and spit out $\dfrac{-q_t}{2\nu_t} +\dfrac{1}{2 \nu_t^2} U_t'U_t$. I can then do a loop for t in 1 through T to calculate each entry in the vector.


The Hessian matrix is the following diagonal matrix:
\begin{align}
\dfrac{\partial^2}{\partial \nu^2} \log f_\theta(u) = \begin{bmatrix} \dfrac{q_1}{2 \nu_1^2}- \dfrac{1}{\nu_1^3} U_1'U_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \dfrac{q_T}{2 \nu_T^2}- \dfrac{1}{\nu_T^3 U_T'U_T} \end{bmatrix}
\end{align}
I will create another function to take $q_t$, $U_t$, $\mu_t$ and $\nu_t$ and spit out $\dfrac{q_t}{2 \nu_t^2} - \dfrac{1}{\nu_t^3}  (U_t-\mu_t)' (U_t-\mu_t)$. Again, during the above loop of length T, I will calculate the entries for the diagonal of this Hessian matrix.

Finally, in the end, I'll return the value (scalar), the gradient vector, and the hessian matrix.

What will the inputs of this function be? Either the vector $\sigma$ of standard deviations or the vector $\nu$ of variance components, U (the vector of random effects), and the list z (from mod.mcml dollar z).  

Something we need is $q_t,t=1,...,T$. z is a list of length T, and each item in the list is a matrix with the number of columns equal to $q_t$.

\subsection{Splitting U into $U_t$ and $\mu$ into $\mu_t$}
The last thing we need to do is split $U$ into $U_1,...,U_T$ and $\mu$ into $\mu_1,...,\mu_T$. The same process should work for both, so let's just look at how we'd do it for $U$.  We know that the first $q_1$ items of U are $U_1$, the next $q_2$ items are $U_2$, etc.  In other words, entries 1 through $q_1$ are $U_1$. Items $q_1+1$ through $q_1+q_2$ are $U_2$, etc. In R pseudo code, this means 
\begin{verbatim}
nrand<-lapply(z,ncol) #get the number of columns from each matrix in list z
unlist(nrand) #chance the list into a vector, since each entry in the list is just a scalar

U1<- U[1 through nrand[1]] #first rule is use 1:nrand[1]
Ut<-U[sum(nrand[1:t-1]+1 through sum(nrand[1:t])]
\end{verbatim} 

Note that we don't need to actually calculate either A or D. We work only through $\nu$.


\section{PQL}
 Before we get started on describing PQL, we're going to do a bit of changing notation because we don't want to have to deal with a constrained optimization.  Recall that $D=Var(u)$. Let $A^2=D$ so that A can have diagonal components that are positive or negative. We use A rather than D so we have unconstrained optimization.  If $\sigma$ is a vector of the distinct standard deviations with components $\sigma_t$, we can write A as a function of $\sigma$ by
\begin{align*}
A= \sum_t E_t \sigma_t
\end{align*}
Recall that $E_t$ has either 0s or 1s on the diagonal to show which random effects go together, and $\sum_t E_t$ is the identity matrix. PQL will tell give us the components of A, and we'll take the absolute values of those components to get the standard deviations.

The other change is that we'll be using $s$ where $u=As$. The purpose of this is to avoid $D^{-1/2}$ in the thing we're trying to optimize.


I will need to do PQL in order to form the importance sampling distribution. There are two ways to do PQL, both of which are described in the  vignette ``re'' from aster. In either case, there is an inner optimization and an outer optimization. The inner optimization is well behaved while the other optimization is a little tougher. I will be using the thing that is not quite PQL but is pretty close and is better behaved.  In this version, the inner optimization finds $\tilde{\beta}$ and $\tilde{s}$ given $X$, $Z$ and $A$. Then, given $\tilde{\beta}$ and $\tilde{s}$, the outer optimization finds $A$.

The default of optim is to minimize, but we'd like to  do maximization. If we just reverse the sign of the function and the gradient, it should do what we want.


The {\bf inner} optimization will be done with the trust function in R. We'd like the inner maximization to be a little more precise because any sloppiness in the inner optimization gets carried into the outer optimization.
The inner optimization maximizes the penalized log likelihood, which is calculated as follows:
\begin{align}
\eta=X\beta +ZAs\\
l(\eta)= Y^T \eta - c(\eta) \\
value= l(\eta)- \dfrac{1}{2} s^Ts
\end{align}

The value of the function is a scalar.  We need to give trust derivatives with respect to $s$ and $\beta$ as well. We're going to express these via the multivariate chain rule, taking advantage of $\eta_i$.

Since
\begin{align}
l(\eta) = \sum_i Y_i \eta_i - c(\eta_i)
\end{align}
then 
\begin{align}
\dfrac{\partial \l(\eta)}{\partial \eta_i} = Y_i-c'(\eta_i) = Y_i -\mu_i
\end{align}
and the off-diagonal is 0. Create  vector $\mu$ from the components $\mu_i$.  Now we can write the following expression:

\begin{align}
%\dfrac{\partial}{\partial s_k} \left[ l(\eta)-\dfrac{1}{2} s^Ts  \right]= \sum_i (Y_i-\mu_i) \dfrac{\partial \eta_i}{\partial s_k} \\
\dfrac{\partial}{\partial \beta_k} \left[ l(\eta)-\dfrac{1}{2} s^Ts  \right] &= \dfrac{\partial l(\eta)}{\partial \beta_k}    \\
&= \dfrac{\partial l(\eta)}{\partial \eta_i} \dfrac{\partial \eta_i}{\partial \beta_k}    \\
&=\sum_i (Y_i-\mu_i) \dfrac{\partial \eta_i}{\partial \beta_k} \\
&=\sum_i (Y_i-\mu_i) X_{ik}
\end{align}


We find the function's derivative with respect to $s$ as follows:
\begin{align}
\dfrac{\partial }{\partial s} \left[ l(\eta) -\dfrac{1}{2} s's   \right] &= \dfrac{\partial l(\eta)}{\partial s} -\dfrac{1}{2} \dfrac{\partial s's}{\partial s}\\
&= \dfrac{\partial l(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial s} -s\\
&= (Y-\mu)' \left[ \dfrac{\partial}{\partial s} ZAs \right] -s\\
&=(Y-\mu)'ZA -s
\end{align}

This gives us the following derivatives:
\begin{align}
\dfrac{\partial}{\partial \beta} \left[ l(\eta)-(1/2)s^Ts \right]&= X^T (Y-\mu)\\
\dfrac{\partial}{\partial s} \left[ l(\eta)-(1/2)s^Ts \right]&= AZ' (Y-\mu)  -s
\end{align}

Lastly, we need the hessian of the penalized likelihood. This matrix can be broken down into four pieces: 
\begin{enumerate}
\item $ \dfrac{\partial^2}{\partial s^2}$
\item $ \dfrac{\partial^2}{\partial \beta^2}$
\item $\dfrac{\partial^2}{ \partial s \; \partial \beta}$
\item $\left(\dfrac{\partial^2}{ \partial s \; \partial \beta}\right) ' =\dfrac{\partial^2}{ \partial \beta \; \partial s}$
\end{enumerate}

We'll start at the top with  $ \dfrac{\partial^2}{\partial s^2}$.
\begin{align}
  \frac{\partial^2}{\partial s^2}  \left[ l(\eta) - (1/2) s's   \right] &=   \frac{\partial}{\partial s} \left[ (Y-c'(\eta))'ZA -s   \right]\\
&=\left[- \frac{\partial}{\partial s} c'(\eta)\right]'ZA - I_q \\
&= \left[-\dfrac{\partial c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial s}   \right] 'ZA - I_q \\
&= \left[ -c''(\eta) ZA  \right]'ZA - I_q \\
&= -AZ' \left[ c''(\eta) \right] ZA - I_q
\end{align}

Note that $c''(eta)$ is a $q\times q$ diagonal matrix with diagonal elements $c''(\eta_i)$.  $I_q$ is the identity matrix of dimension $q$.  This makes  $ \dfrac{\partial^2}{\partial s^2}$ a $q \times q$ matrix.

Next up is  $ \dfrac{\partial^2}{\partial \beta^2}$, which is a $p \times p$ matrix.
\begin{align}
  \dfrac{\partial^2}{\partial \beta^2}  \left[ l(\eta) - (1/2) s's   \right] &=   \dfrac{\partial}{\partial \beta} \left[ X' (Y-c'(\eta))  \right]\\
&= \dfrac{\partial}{\partial \beta} \left[ X' Y-X'(c'(\eta))  \right] \\
&= \dfrac{\partial}{\partial \beta} \left[ -X'(c'(\eta))  \right] \\
&=-X  \left[' \dfrac{\partial}{\partial \beta} c'(\eta)  \right] \\
&=-X'  \left[ \dfrac{\partial  c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial \beta}  \right] \\
&=-X'  \left[ c''(\eta)\right]  X   \\
\end{align}

Next is the $p \times q$ mixed partial $\dfrac{\partial^2}{  \partial \beta \; \partial s}$.
\begin{align}
\dfrac{\partial^2}{ \partial \beta \;  \partial s}  \left[ l(\eta) - (1/2) s's   \right] &= \dfrac{\partial}{\partial \beta} \left\{  \left[ Y-c'(\eta)  \right]' ZA  \right\}\\
&= \dfrac{\partial}{\partial \beta} \left\{Y'ZA-  \left[ c'(\eta)  \right]' ZA  \right\} \\
&=- \dfrac{\partial}{\partial \beta} \left\{  \left[ c'(\eta)  \right]' ZA  \right\} \\
&=- \left[  \dfrac{\partial  c'(\eta)}{\partial \beta}  \right]' ZA  \\
&=-   \left[  \dfrac{\partial  c'(\eta)}{\partial \eta} \dfrac{\partial \eta}{\partial \beta}  \right]' ZA   \\
&=-   \left[ c''(\eta) X  \right]' ZA   \\
&=-X'   \left[ c''(\eta)   \right] ZA  
\end{align}


And last we have the $q \times p$ mixed partial $\dfrac{\partial^2}{  \partial \beta \; \partial s}= AZ' [c''(\eta)] X.$  These four pieces specify the hessian matrix for the penalized likelihood. Trust can now perform the inner optimization to find $\tilde{\beta}$ and $\tilde{s}$. Trust will maximize the penalized likelihood.





The {\bf outer} optimization will also be done using optim with the default method of Nelder-Mead. This requires just the function value. 

The outer optimization's function is evaluated by
\begin{align}
\tilde{\eta} &= X \tilde{\beta} +ZA \tilde{s}\\
l(\tilde{\eta}) &= Y^T \tilde{\eta} - c(\tilde{\eta})\\
A&= \sum_k E_k \sigma_k\\
W&=c''(\eta)=diag(c''(\eta_i))\\
maxvalue&= l(\tilde{\eta}) - \dfrac{1}{2} \tilde{s}^T \tilde{s} - \dfrac{1}{2} \log det \left( AZ' W ZA +I  \right)\\
minvalue&= - maxvalue
\end{align}
Again, we have to minimize the negative value of the penalized quasi likelihood in order to maximize the value of it.

We do need to be careful about the determinant.  I did a cholesky decomposition.(Maybe it would be better to do a qr or svd decomposition, but I couldn't figure out whether the q matrix in the qr decomposition would have a determinant of positive or negative one. QR is supposed to be more numerically stable, but slower. Charlie used a cholesky decomposition in aster's PQL so I thought it should be good enough).  Let $AZ' W ZA +I=M=LL'$, where $L$ is the lower triangular matrix resulting from the decomposition.
\begin{align}
.5 \log det(M)) &=.5 \log det(LL')\\
&= .5 \log (det(L))^2 \\
%= 2*.5 \log det(L) \\
&=  \log det(L) \\
&= \log \left( \prod diag(L) \right) \\
&= \sum \log diag(L)
\end{align}

If I need to be more careful and more stable, I can use log1p, which is a function that is good for calculating $\log (1+ a)$ where $a$ is a tiny number. Let $M=AZ' W ZA +I$. Let $O$ be orthogonal and $D$ be diagonal with $\lambda_i$ on the diagona, where $\lambda_i$ are the square roots of the eigenvalues of $M'M$ and $MM'$l. Then that  $AZ' W ZA +I$ can be decomposed in the following way, 

\begin{align*}
 &AZ' W ZA = ODO' \\
&\Rightarrow AZ' W ZA +I  = O (I+D) O' \\
&\Rightarrow  det \left( AZ' W ZA +I \right) = 1 *  \prod_{i=1}^n (1+\lambda_i) *1\\
&\Rightarrow \log  det \left( AZ' W ZA +I \right) = \sum \log(1+\lambda_i) = \sum log1p(\lambda_i)
\end{align*}

The outer optimization will use $\tilde{\beta}$ and $\tilde{s}$ provided by the inner optimization, but it does not return them. I'll want to keep track of the very last  $\tilde{s}$ for generating the random effects, so I'm going to store them in the parent environment with the 
\begin{verbatim}<<- \end{verbatim}


The point of doing PQL is to construct a decent importance sampling distribution. Thus, the estimates don't have to be perfect.  It is possible that one of the $\sigma_k$ will be 0 according to PQL. If this happens, then I'll just use $\sigma_k = .01$ or something like that for the importance sampling distribution.


\section{generating random effects (genRand)}

This will be hidden from the user. It will be called by the model fitting function. 

To approximate the log likelihood, we need simulated random effects.  PQL will give us estimates for the $\sigma_t$s and  the s's. Since we have the $E_t$, we can construct A, which will be useful because it is is the cholesky decomposition for D =AA.

Let $s^*$ and $\sigma^*$ be the vectors of PQL estimates. Then we can ``unstandardize'' our random effects this way:
\begin{align}
u^*&=A^*s^*\\
&= \left(\sum E_t \sigma^*_t \right) s^*
\end{align}

Suppose we want to generate random effects $u_k,k=1,...,m$ where $u_k$ is a vector with length q (the number of random effects in the model). Then for each k, draw a sample of size q from the standard normal distribution. Call this sample $\breve{u}_k$. Then we scale it
\begin{align}
u_k = \breve{u}_k A^* + u^*
\end{align}
so that we are effectively drawing our sample from $N(u^*,A^*A^*)$. We are using this distribution because it is our best guess of the distribution of our random effects.

%Then sample from $N(u^*, D^*)$ $m$ times to generate random effects $u_k,k=1,...,m$.  To create a sample from $N(u^*, D^*)$, I'm actually going to get $\sum_{t=1}^T t$ samples from $N(0,1)$ (call these $\breve{u}_k$) then scale this vector to be from $N(u^*, D^*)$. Using the Cholesky decomposition,  $D^* = L L^*$. Then the scaling works as follows:
%\begin{align}
%u_k = \breve{u}_k L + u^*
%\end{align}
%Note: we are using $u^*$ because we want to generate the $u$s so that they are as close as possible to their most likely values. The $u^*$s are the best guess of $u$ that we have.

Lastly, we need an expression for $\log \tilde{f} (u_k)$. We can write it as a function proportional to the normal distribution with mean $u^*$ and variance $D^*=A^*A^*$.  In other words, we will use the distRand function with $u^*$ as the mean vector and $D^*$ as the variance matrix.


\section{objective function: approximated log likelihood}
This will be hidden from the user. It will be called by the model fitting function.

In order to maximize the approximated log likelihood using trust, I will need an objective function that returns the value, the gradient vector, and the Hessian matrix of the log likelihood.  This objective function will get $\log f_\theta (u_k)$, $c(\eta_i)$, each of their gradient vectors, and each of their Hessian matrices to plug into these expressions.    Denote\\
\begin{align}
b_k &=  \log f_\theta (u_k,y)- \log \tilde{f} (u_k) \\
&= \log f_\theta (u_k) + \log f_\theta (y|u_k) - \log \tilde{f} (u_k) \\
%&= \log f_\theta (u_k) +\sum_i Y_i \eta_i -\sum_i c(\eta_i) - \log \tilde{f} (u_k)
\end{align}


For computational stability, we'll set $a=max(b_k)$. Then the value is expressed as:\\
\begin{align}
\log f_{\theta,m} = a+ \log \left[ \dfrac{1}{m}  \sum_{k=1}^m e^{b_k-a} \right]
\end{align}

Define the weights as:\\
\begin{align}
v_\theta(u_k,y) = \dfrac{e^{b_k-a}}{ \sum_{k=1}^m e^{b_k-a}} 
\end{align}

The gradient vector is: \\
\begin{align}
G &= \sum_{k=1}^m \nabla \log f_\theta (u_k,y) v_\theta(u_k,y)\\
&= \sum_{k=1}^m \nabla \left[  \log f_\theta(y|u_k) + \log f_\theta (u_k)  \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \nabla \log f_\theta(y|u_k) + \nabla\log f_\theta (u_k)  \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \;  \dfrac{\partial}{\partial \nu} \log f_\theta(y|u_k) \right] v_\theta(u_k,y)+ \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(u_k) \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \; 0 \right] v_\theta(u_k,y)+ \left[ 0 \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right] v_\theta(u_k,y)\\
&= \sum_{k=1}^m  \left[ \dfrac{\partial}{\partial \beta} \log f_\theta(y|u_k) \; \; \; \; \dfrac{\partial}{\partial \nu} \log f_\theta(u_k) \right] v_\theta(u_k,y)
\end{align}

Then the Hessian matrix is: \\
\begin{align}
H = \sum_{k=1}^m \nabla^2 \log f_\theta (u_k,y) v_\theta(u_k,y) +  \sum_{k=1}^m \left[ \nabla  \log f_\theta (u_k,y)  \right] \left[ \nabla  \log f_\theta (u_k,y)  \right]^T  v_\theta(u_k,y) -G G^T
\end{align}
Everything in this has already been defined except:
\begin{align}
\nabla^2 \log f_\theta (u_k,y) &=  \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k,y) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k,y)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k,y)  \end{bmatrix} \\
&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k) \\ \dfrac{\partial^2}{\partial \beta \: \partial \nu} \log f_\theta (y|u_k)  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (y|u_k)  \end{bmatrix} \\
&= \begin{bmatrix} 0 & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} + 
\begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0 & 0  \end{bmatrix} \\
&= \begin{bmatrix} \dfrac{\partial^2}{\partial \beta^2} \log f_\theta (y|u_k) & 0 \\ 0  & \dfrac{\partial^2}{ \partial \nu^2} \log f_\theta (u_k)  \end{bmatrix} 
\end{align}

\section{checks}
I'll need to perform some checks to make sure that I've coded these right. Finite differences is one.


\section{summary of model}
Typing ``summary(mod)'' will give more detailed info of the model. This will be broken into two pieces (as is usually done for summaries): there will be the summary.mcml and print.summary.mcml. We split this up because we might have a lot of extra stuff in the summary that we don't necessarily want printed each time. So, when a user types ``summary(mod)'', only the  basic information is automatically printed because that's all that most users want. If they want some of the more sophisticated stuff, they can dig into the summary list and find it.

The summary will do all the calculations and its value will be a list of the following:
\begin{itemize}
\item call
\item the variance estimate(s)
\item a matrix with the predictor in the first column, the estimated coefficient in the second column, the standard error in the third column, the z value in the fourth column, and the two-sided pvalue  in the fifth column. (note that we are using the standard normal distribution for the reference distribution, not a t distribution like I initially assumed. This is because all inference is asymptotic.)
\item the evaluated Monte Carlo log likelihood along with its first and second derivative
\item maybe other things that I haven't thought of
\end{itemize}

A note on the standard errors: to calculate the standard errors, we take the MCLA hessian matrix, invert it, take the diagonal elements, and square root them. It's possible that the hessian will be noninvertible if it is close enough to singular that the computer thinks it's singular.  Then the standard errors will all be infinite. We also need to warn the user why this is happening.

Then print.summary.mcml will print these things:
\begin{itemize}
\item call
\item the variance estimate(s)
\item a matrix similar to the output of summary.glm, which will be put through printCoefmat in order to get the significance stars that we're used to. We'll have the predictor in the first column, the estimated coefficient in the second column, the standard error in the third column, the z value in the fourth column, and the two-sided pvalue  in the fifth column, the significance stars, and the significance legend.
\item maybe other things that I haven't thought of. I'll probably realize more once I get coding.
\end{itemize}

I think for the printCoefmat to work, the fixed effects matrix needs to look like
\begin{verbatim}
            Estimate Std. Error z value     Pr(>|z|)
(Intercept)        2       0.50       4 6.334248e-05
x1                 2       0.25       8 1.244192e-15
\end{verbatim}
(or at least it worked when I had these as the column names and didn't work before that).

Note that the summary and print.summary functions will be S3 generic functions.  What this means is the user will type ``summary(mod)'' and summary is a generic function. R checks the class of the model $mod$ and then automatically uses the summary and print.summary functions for that class of objects. This also might affect the way that I export the function and document it in the manual.  I think that I'll be ok if I copy parts of his summary.aster and print.summary.aster. I need to read about generics more and will check out the hints library.

\section{likelihood ratio test}
This is another S3 generic function that the user can choose to implement if they'd  like  to do likelihood ratio tests for nested models. Eventually I'll want this to handle an arbitrary number of models, but for now we'll stick to comparing two models: $mod2$ nested in $mod1$.  I will assume these models have already been fit. 


 There are a few ways that the two models could differ. In other words, we could be testing:
 \begin{enumerate}
 \item whether one or more fixed effects are zero.
 \item whether one variance component is zero.
 \item whether multiple variance components are zero.
 \item whether one or more fixed effects is zero and one or more variance components are zero. 
\end{enumerate}
The hypothesis testing procedure and method for calculating pvalues are different for each of these cases, so I'll go through each one in the following subsubsections.

\subsection{testing whether one or more fixed effects are zero}
Consider the  case of testing whether one or more fixed effects are zero and the random effects are the same between the two models. I think we just do a likelihood ratio test as we're familiar with for linear models.   Let the number of fixed effect parameters in the larger model be $p_1$  and the number in the nested model be $p_2$. I will need the log likelihood values of the models outputted (from the main function at the same time as when I output the MCMLEs and the Fisher information). The log likelihood is simply the ``value'' from the objective function that trust uses. Let's call the log likelihood from the larger model  $l_1$ and the log likelihood from the nested model  $l_2$. 

Then the likelihood ratio test statistic is
\begin{align}
t_{LRT}= -2l_1+2l_2
\end{align}
and this follows a $\chi^2$ distribution with $p_1-p_2$ degrees of freedom. The calculation
\begin{align}
P(t_{\chi^2_{p_1-p_2}>LRT} ).
\end{align}
gives us a two-sided pvalue to fit with the two-sided alternative hypothesis.

 
 \subsection{testing whether one variance component is zero}
  Hypothesis testing for one variance component is easy but not what most people expect.
  In this setup, the two models have the exact same fixed effects. The only difference is the larger model has one more random effect $\nu_{t_1}$. This means we want to test whether $\nu_{t_1}=0$. A variance component must be nonnegative, meaning the alternative hypothesis is that $\nu_{t_1}>0$. In other words, the hypotheses are:
 \begin{align}
 H_0: \nu_{t_1}=0\\
 H_1: \nu_{t_1}>0.
 \end{align}
 Note the alternative hypothesis is one-sided. This means we need to calculate a one-sided pvalue. Let $t_{LRT}=2l_2-2l_1$. If we naively follow the pvalue calculation of
 \begin{align}
P( \chi^2_{1}>t_{LRT})
\end{align}
 we will end up calculating a two-sided pvalue. To fix this, we cut this pvalue in half. In other words, use the standard normal distribution $Z$ and think of calculating the one-sided pvalue this way:
 \begin{align}
 P( Z>\sqrt{|  t_{LRT} |})
 \end{align}
 
 However, this is not obvious; I never would have thought about having a test statistic of
 \begin{align}
 \sqrt{2 | l_2-l_1   |   }.
 \end{align}
 
 \subsection{testing whether multiple variance components are zero}
 In this case, the fixed effects are identical between the larger and nested model. The only difference is that the larger model has more than one additional variance components. This gets a lot more complicated (for example, there is no clear way to count parameters so calculating the degrees of freedom is out the window). Luckily, someone's worked out the details already. Charlie suggested a few papers to look at. I'll read about and add this a bit later.  Charlie is hoping that we'll have a mixture of $\chi^2$ distributions, such as
 \begin{align}
 \dfrac{1}{2}P (\chi^2_1> t_{LRT}) +\dfrac{1}{4}P (\chi^2_2> t_{LRT}).
 \end{align}
 The reason this is complicated is because the variance components are restricted to be nonnegative. This means that the likelihood is only defined for nonnegative variance components. Then differentiating the likelihood at $0$ becomes tricky because that's the boundary.

 \subsection{testing whether one or more fixed effects is zero and one or more variance components are zero}\label{sec:combotest}
 This seems very complicated. I don't know if it's any more complicated than the previous case of testing multiple variance components. I'll have to think about it later when I have time.
 
 \subsection{determining the hypothesis test}
 To follow convention, this command will be called ``anova'' and the two arguments will be the two models to be compared.  The command would look like
\begin{verbatim}
anova(mod1,mod2)
\end{verbatim} 

 R will first need to figure out if the fixed effects are different and, if they are, which model is the larger model.
\begin{verbatim}
if(length(coef(mod1))>length(coef(mod2)))
      {bigmod<-mod1;  smallmod <-mod2} 
if(length(coef(mod1))<length(coef(mod2)))
      {bigmod<-mod2; smallmod <-mod1}
\end{verbatim}
Next, if there is a difference in the fixed effects, I'm going to make sure the fixed effects of the small model are nested in those of the big model. Otherwise, produce an error. Then, continue with the testing. Note: this check for nesting isn't perfect, but no other anova checks which model is bigger and whether they're nested.
\begin{verbatim}
if(big mod is defined) {
      pnames1<-names(coef(bigmod))
      pnames2<-names(coef(smallmod))
      if(sum(pnames2 %in% pnames1) != length(pnames2)) {
            stop("The models you provided are not nested.")}

     if(the variance components differ between the two models){
             check big model has more variance components (ow: error)
             check variance components of the small model are nested (ow: error)
             calculate pvalue according to section 3.3.4. return it.
      }
      if(the variance components do not differ between the two models){
            calculate pvalue according to 3.3.1. return it.
      }
}
\end{verbatim}
If and only if we've gotten to the end of this chunk of code without returning anything, bigmod has not been defined, meaning the fixed effects are same. Therefore, we now need to figure out whether the variance components differ by one or by more than one. In order to compare the number of random effects, we need to figure out how to get the number of variance components from each model. I  don't yet have a solid grasp of how formula works its magic, but I have the impression that I can count the number of model matrices returned in order to figure out the number of variance components because there is a model matrix for each one.

\begin{verbatim}
T1<- number of variance components for mod1
T2<- number of variance components for mod2
if(T1>T2){call mod1 the bigmod and mod2 the smallmod}
if(T1<T2){call mod2 the bigmod and mod1 the smallmod}
if(bigmod is defined){
     check  small model is nested in the big model (ow: error)
     (maybe do this by looking at the call?)
     if(T1==T2+1 || T2==T1+1) 
           {calculate and return pvalue according to sec 3.3.2}     
     calculate and return pvalue according to section 3.3.3
     }
if still haven't returned anything, produce error.     
\end{verbatim}	

Why the last error? If we have gotten to this point in the code with nothing returned, it ends up that the number of variance components in each model are equal, meaning the user has made some kind of mistake. Either the user provided two identical models or they provided non-nested models (e.g. they have the same number of variance components, but different components in each model). Either way, we can't help them.





This command would return something that first reminds the user what predictors are in each model, then has a table. Each model would have one row of the table. The columns would contain the model name,  the log likelihood for each model, the number of parameters in the model. Then the next columns would have one entry each: the calculated difference between the log likelihoods, the calculated difference between the number of parameters, and the pvalue of the test. 

%\section{AIC}
%Typing 
%\begin{verbatim}
%AIC(mod)
%\end{verbatim} 
%will return the AIC of the model. To calculate AIC, I'll need the total number of parameters ($p+T$)  and the Monte Carlo log likelihood evaluated at the MCMLEs $l$. Then the AIC is
%\begin{align}
%2(p+T) - 2 l 
%\end{align} 


\section{confidence intervals}
The user can implement this command to create confidence intervals after  fitting a model $mod$ using the main function.  This is an S3 generic.The command would look like
\begin{verbatim}
confint(mod,parm,level=.95)
\end{verbatim}
The only required argument would be the fitted model (the first argument). The third argument (the confidence level) has a default of $.95$.

If the second argument is omitted, confidence intervals would be created for all of the parameters. There are two options to calculate confidence intervals for a subset of the parameters. The user can provide either  the names of the coefficients or a vector with length equal to the number of parameters (entries being 1 if they would like that intervals for that parameter and 0 otherwise).  The code to do this is taken from ``confint.lm'' and is:
\begin{verbatim}
function (object, parm, level = 0.95, ...) 
{
    cf <- coef(object)
    pnames <- names(cf)
    if (missing(parm)) 
        parm <- pnames
    else if (is.numeric(parm)) 
        parm <- pnames[parm]
        stopifnot(parm %in% pnames)
        
        rest of the code to actually do the confidence intervals goes here
        }
\end{verbatim}
What this says: write down the coefficient names of the object we're given. If parm is missing, we're going to assume they want to create intervals for all the parameters.  If parm is numeric (a vector of 0s and 1s), use that to select the parameter names we want and call that selection parm.  Finally, stop the whole process if parm (the parameter names we want intervals for) don't appear in the model.

The form for a confidence interval is the point estimate plus or minus the standard error of the point estimate times some cutoff.  The point estimate and the standard error are from the summary of the model. The reference distribution used for the cutoff is the standard normal. This will produce an asymptotic confidence interval.

The output would either be a vector of length 2 (if creating confidence interval for only one parameter) or a matrix with 2 columns (one for the lower bound of the confidence interval, one for the upper bond). The column names will be ``$(100-level)/2$'' and ``$50+level/2 $.'' (in the default case, this is``$2.5\%$'' and ``$97.5\%$'').

The only potential hangup is when creating confidence intervals for the variance components $\nu_t, t=1,...,T$.  We know that $\nu_t >0$ for all $t=1,...,T$. We'll find this boundary again makes things complicated. I don't yet know how to deal with this.  

To illustrate the problem, consider the scenario that the margin of error for $\nu_t$ is greater than the estimate of $\nu_t$ itself. It wouldn't make sense to produce a confidence interval with a negative lower bound. It could make sense to truncate the interval so that the lower bound starts at 0 and the upper bound remains untouched.  

I thought for three or four minutes about a one sided confidence interval, but I don't think that captures what we want. We want the range of all likely values for the variance component, not just an upper bound.


\section{prediction intervals}
This is another S3 generic. The user can implement this command to create prediction intervals after  fitting a model $mod$ using the main function. Still learning about these. I currently know almost nothing. What I do know is that I will need the Monte Carlo log likelihood along with its first and second derivatives. I'll get to practice my first-year-theory delta method skills.

Charlie says that neither predict.glm nor predict.glmmPQL do prediction intervals. The former produces parameter estimates with standard errors. The latter does predictions but does not produce a standard error, so no interval can be formed.

One wrong thing to do: once you find the MCMLEs, you simply plug those into the distribution for $U|Y$ and pretend those parameter estimates have no variability and are  correct. Then the random effect has a normal distribution and it'd be easy to find quantiles. The only problem is this is wrong.

We are hoping to find a way to use Fisher information for the Monte Carlo MLE. Hopefully we can do something more similar to the usual asymptotics.

A last resort is a parametric bootstrap. I haven't spent much time thinking about this, but I think the way this involves plugging parameter estimates  into $\log f_\theta(y|u)$ and $\log f_\theta(u)$. Then you repeat: draw a value of $u$, plug that into $\log f_\theta(y|u)$, draw a value of $y$. I say this is a last resort  because having two Monte Carlo calculations will take a long time. 


\end{document}