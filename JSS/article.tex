\documentclass[article]{jss}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Christina Knudson\\ University of St. Thomas \And 
        Charles Geyer\\University of Minnesota \And 
        Galin Jones\\University of Minnesota}
\title{ Likelihood-Based Inference for Generalized Linear Mixed Models: Inference with Package \pkg{glmm}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Christina Knudson, Charles Geyer, Galin Jones} %% comma-separated
\Plaintitle{A Capitalized Title: Something about a Package foo} %% without formatting
\Shorttitle{\pkg{glmm}: Likelihood-Based Inference for Generalized Linear Mixed Models} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
The \proglang{R} package \pkg{glmm} enables all  likelihood-based inference  for generalized linear mixed models with a canonical link by approximating the entire likelihood function and two derivatives. The model fitting function of \pkg{glmm} calculates and maximizes the Monte Carlo likelihood approximation to find Monte Carlo maximum likelihood estimates for the fixed effects and variance components. Additionally, the value, gradient vector, and Hessian matrix of the MCLA are calculated at the Monte Carlo maximum likelihood estimates. \pkg{glmm} also produces the variance-covariance matrix of the estimates, confidence intervals for the parameters, and Monte Carlo standard errors of the Monte Carlo maximum likelihood estimates.
}
\Keywords{generalized linear mixed models, maximum likelihood, likelihood-based inference, mixed models, likelihood approximation, Monte Carlo}
\Plainkeywords{generalized linear mixed models, maximum likelihood, likelihood-based inference, mixed models, likelihood approximation, Monte Carlo}
 %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Christina Knudson\\
  Department of Mathematics\\
  Faculty of  Statistics\\
 University of St. Thomas\\
2115 Summit Avenue, Saint Paul, Minnesota\\
  E-mail: \email{christina@umn.edu}\\
  URL: \url{http://cknudson.com/}
}
%\Address{
% Galin Jones\\
%School of Statistics\\
%  Faculty of Statistics\\
% University of Minnesota\\
%224 Church Street Southeast, Minneapolis, Minnesota \\
%  E-mail: \email{galin@umn.edu}\\
%  URL: \url{http://users.stat.umn.edu/~galin/}
%}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}
The generalized linear mixed model (GLMM) is an extension of both the generalized linear model and the linear mixed model; the model incorporates  fixed and random effects as well as a response from an exponential family.   GLMMs were first discussed by \citet{stiratelli:laird:ware:1984} and are now  used in a variety of disciplines. Despite their widespread use, traditional methods of frequentist likelihood-based inference are not generally available. The challenge lies in the likelihood function for GLMMs: because the likelihood cannot depend on the random effects, the likelihood is an  integral that is often intractable. Due to this challenge, most methodology and software for  GLMMs perform little more than maximum likelihood (such as Monte Carlo EM)  or do not perform likelihood-based inference at all (such as penalized quasi-likelihood). \\

Our goal is to enable all types of frequentist likelihood-based inference. This includes (but is not limited to) calculating Fisher information, performing hypothesis tests, and constructing confidence intervals.  To perform all types of frequentist likelihood-based inference, the entire likelihood function is necessary.\\


With this goal in mind, we have created the  \proglang{R} package \pkg{glmm} \citep{glmm:package} that approximates the entire likelihood function for a GLMM with either a Bernoulli or Poisson response. This package uses  the method of
 Monte Carlo likelihood approximation (MCLA) \citep{geyer:1994, geyer:thom:1992,  sung:geyer:2007}. 
This method relies on the  choice of an importance sampling distribution,  a distribution from which simulated random effects are drawn. These generated random effects are then used to approximate the entire likelihood function. 
Because MCLA approximates the entire likelihood, \pkg{glmm}  enables  every type of likelihood-based inference. \\

%For more details on MCLA, see \ref{sec:mclageneral}.

Before the release of \proglang{R} package \pkg{glmm}, no publicly-available software  implementing MCLA  produced accurate maximum likelihood estimates for models of non-trivial complexity. For example, the \texttt{bernor}  package \citep{sung:geyer:2007} implements MCLA but cannot accurately perform maximum likelihood for the benchmark data set from a salamander mating experiment (described in section \ref{sec:examples}). 
%The  absence of MCLA software highlights a   challenge from which the procedure suffers:  finding an importance sampling distribution that works well in practice.  
%We propose  an importance sampling distribution to be used in implementing MCLA for GLMMs (Equation {eq:ftwiddle});  
We present   \pkg{glmm}  and demonstrate in section \ref{sec:howto} how to use the package to perform maximum likelihood, test hypotheses, calculate confidence intervals, and calculate Monte Carlo standard errors for the estimates.  




\section{Monte Carlo Likelihood Approximation}

MCLA is a  Monte Carlo method for approximating the entire likelihood function; it was first proposed by \citet{geyer:1990} for models with unnormalized densities, then extended to models with normalized densities and random effects \citep{thom:guo:1991}, and then extended again to models with unnormalized densities and random effects \citep{gelf:carl:1993}.
 We focus on  MCLA  for GLMMs with random effects.  \\

MCLA is powerful  because it enables any type of likelihood-based inference.  Maximum likelihood is of special interest; the maximizer of the Monte Carlo likelihood approximation is called the Monte Carlo maximum likelihood estimate (MCMLE). Because MCLA is a Monte Carlo method, the accuracy of the likelihood approximation and inferences based on the likelihood approximation  can be improved by increasing the size of the Monte Carlo sample used to calculate the likelihood approximation. \\


Prior to \pkg{glmm}, MCLA for GLMMs was   studied by \citet{sung:geyer:2007}, who  focused on an MCLA implementation with an importance sampling distribution constructed independently of the observed data. \citet{sung:geyer:2007} prepared the \proglang{R} package \texttt{bernor} that  fits maximum likelihood estimates for a GLMM with a Bernoulli response; their package uses an importance sampling distribution that does not depend on the observed data. Though their package can perform maximum likelihood for simpler models,  it cannot find MLEs for a model as complicated as the salamander model (described in section \ref{sec:examples}).  \\

This highlights the challenge of  finding an importance sampling distribution that performs well in practice. Though MCLA theory indicates any importance sampling distribution should suffice as long as its support contains the support of the target distribution, many importance sampling distributions require so much computing power that modern computers cannot perform  likelihood-based inference in practice.
  This practical problem is  illustrated by the absence of MCLA software that can produce MLEs for nontrivial problems. In response, we present an importance sampling distribution \citep{mythesis} and the \proglang{R} package \pkg{glmm}.\\

\subsection{Calculation of the Monte Carlo likelihood approximation}

Let $Y \in \mathcal{R}^n$ be a response vector and let $U \in \mathcal{R}^q$ be a vector of unobservable random effects. The MCLA calculation requires the density for the joint distribution of the observed data  and random effects, $f_\theta(u,y)$. Let  $\beta \in \mathcal{R}^k$ be a vector of the fixed effects and let $f_\beta(y|u)$ denote the density for the observed data conditional on the random effects. Let $\nu$ be a vector of variance components and let $f_\nu (u)$ denote the density for the random effects. Then the joint density can be expressed as the product 
\begin{align}
f_\theta (u,y) = f_\beta(y|u) \, f_\nu (u).
\end{align}


Let $\tilde{f}(u)$ be a nonnegative function whose support contains the support of the $f_\theta(u,y)$, where $\tilde{f}(u_k)$ does not depend on $\theta$.  The  importance sampling distribution used in \proglang{R} package \texttt{glmm} is described in section \ref{sec:myimplem} and by \citet{mythesis}.  Let\hbox{ $u_k,$ $k=1,$ $\ldots,$ $m$}, be vectors of length $q$ drawn from  $\tilde{f}(u_k)$.  
Then the Monte Carlo  likelihood approximation is
\begin{align}
L_{m}(\theta|y) &=   \dfrac{1}{m} \sum_{k=1}^m  \dfrac{ f_\theta(u_k,y)   }{\tilde{f}(u_k)}_. \label{eq:MCLAval}
\end{align}
Because the function in Equation \ref{eq:MCLAval} approximates the entire likelihood function, it can be used to conduct any likelihood-based inference.\\


\subsection[Implementation of MCLA in glmm]{Implementation of MCLA in  \pkg{glmm}} \label{sec:myimplem}

The importance sampling distribution used in \pkg{glmm} depends on the observed data and is specified in detail by \citet{mythesis}. It is a mixture of three distributions: two normal distributions and a t distribution. The t distribution is centered at 0 with a covariance matrix that depends on penalized quasi-likelihood (PQL) estimates of the variance components. The first normal distribution is centered at the PQL predictions of the random effects and has a variance matrix that depends on the PQL estimates of the variance components. The second normal distribution is  centered at the PQL predictions of the random effects and has a covariance matrix based on the Hessian of the  penalized likelihood from PQL.  More specifically, the Hessian of the log density of the last distribution matches the Hessian of the log density of the target distribution $f_\theta(u,y)$. 





\subsection{Asymptotic properties of the Monte Carlo likelihood approximation}


The asymptotic properties of the likelihood approximation as the Monte Carlo sample size increases have been studied in generality  \citep{geyer:1994, geyer:thom:1992} and   for the specific implementation in \pkg{glmm} \citep{mythesis}.  \citet{geyer:1994} presents conditions under which  the Monte Carlo likelihood approximation converges almost surely to the exact likelihood for any single parameter value.  In addition to studying the point-wise convergence,  we can also consider  the convergence of the entire likelihood function.
\citet{geyer:1994} shows the Monte Carlo likelihood approximation converges almost surely to the likelihood function for GLMMs specified with  unnormalized densities as long as a Wald-like integrability condition is met.
 Additionally, if the parameter space can be compactified, the MCMLE converges to the MLE almost surely \citep{geyer:1994}. \citet{geyer:1994} also shows that the Monte Carlo profile likelihoods converge to the exact profile likelihoods almost surely, and no additional regularity conditions are required.\\





\section{Examples}\label{sec:examples}
We  illustrate \pkg{glmm} with two data sets: the salamander data set serves as an example for modeling binomial-distributed data while the radish flower data set serves as an example for modeling Poisson-distributed data. This section provides background information on  the research conducted and Section \ref{sec:howto} demonstrates the analysis using \pkg{glmm}.

\subsection{The salamander data}\label{sec:salex}

Researchers at the University of Chicago conducted an experiment on a single species of salamanders  in 1986.
 \citet[Section 14.5]{mcc:nelder:1989} presented the experiment and data, and \citet{karim:zeger:1992} proposed a model they call ``Model A.'' The salamander data and model have become a benchmark in the GLMM universe;  the data have been modeled by many researchers including \citet{booth:hobert:1999},  \citet{bres:clay:1993},   \citet{karim:zeger:1992}, \citet{mcc:nelder:1989},  \citet{schall:1991},  \citet{sung:geyer:2007}, and \citet{wolfinger:oconnell:1993}.\\




Before the experiment began, female salamanders and male salamanders of the same species were collected from two locations. The salamanders were categorized into populations named ``Rough Butt'' and ``White Side'' based on their location of origin. The scientific goal was to determine whether salamanders were more likely to mate with those from their own population or whether they were just as likely to mate with salamanders from either population. More specifically, scientsts sought to compare the odds of mating for each type of  cross: female Rough Butt salamanders and male Rough Butt salamanders (denoted RR), female Rough Butt salamanders and male White Side salamanders (denoted RW), female White Side salamanders and male Rough Butt salamanders (denoted WR), and female White Side salamanders and male White Side salamanders (denoted WW).    \\

Scientists ran an experiment three times. Each experiment consisted of trials conducted on two closed groups of 20 salamanders. That is, each experiment was conducted on 40 salamanders that were split into two closed groups.  Each group contained 5 female Rough Butts, 5 female White Sides, 5 male Rough Butts, and 5 male White Sides.  Each trial consisted of placing a  female salamander and a  male salamander in an isolated space together, then observing the binary response of interest: whether the salamanders mated.  Each female salamander participated in six trials with male salamanders from her closed group:  three trials with male White Side salamanders and three trials with male Rough Butt salamanders. Scientists paired salamanders from the same group; inter-group trials were not conducted. Thus, 60 trials were conducted on each closed group, each experiment consisted of 120 trials, and the overall dataset contains binary responses from 360 trials. \\


 To model these data with a GLMM,  ``Model A'' \citep{karim:zeger:1992} proposes
a random effect for each female salamander; a random effect for each male salamander; a fixed effect predictor for each of the four types of cross; and a Bernoulli response of whether the pair of salamanders mated, dependent on the type of cross, the female random effect, and the male random effect. 
 Each salamander's random effect is assumed to be independent of the others.  The male salamanders' random effects share one variance component  while the female salamanders' random effects share another variance component. These two variance components are later referred to as $\nu_M$ and $\nu_F$. The males' random effects are crossed with the females' random effects. Though the same salamanders were used in the first two experiments, the data are traditionally modeled assuming that new salamanders were used in each experiment \citep{booth:hobert:1999,  karim:zeger:1992,  mcc:nelder:1989}.\\

 For your convenience, the \texttt{salamander} dataset is  included in the \texttt{glmm} package. The first \proglang{R}  command shown below gives us access to the \texttt{glmm} package and and all of its commands. The second line of code gives us access to the \texttt{salamander} data frame.  The next three commands help us begin to understand the data. We have four variables: \texttt{Mate}, \texttt{Cross}, \texttt{Female}, and \texttt{Male}. The summary shows us \texttt{Mate} is numeric, \texttt{Cross} is a factor with four levels, \texttt{Female} is a factor, and \texttt{Male} is a factor. 

\begin{CodeChunk}
\begin{CodeInput}
library(glmm)
data(salamander)
names(salamander)
\end{CodeInput}
\begin{CodeOutput}
[1] "Mate" "Cross" "Female" "Male"
\end{CodeOutput}
\begin{CodeInput}
head(salamander)
\end{CodeInput}
\begin{CodeOutput}
   Mate Cross Female Male
 1    1   R/R     10   10
 2    1   R/R     11   14
 3    1   R/R     12   11
 4    1   R/R     13   13
 5    1   R/R     14   12
 6    1   R/W     15   28
\end{CodeOutput}
\end{CodeChunk}

The variable \texttt{Mate} tells us whether the pair of salamanders mated: the value is $1$ if they successfully mated and $0$ if they did not. The variable \texttt{Cross} describes the type of female and male salamander. For example, \texttt{Cross = W/R} indicates a White Side female was crossed with a Rough Butt male. The variable \texttt{Female} contains the identification number of the female salamander, and the variable \texttt{Male} contains the identification number of the male salamander.\\ 



\subsection{The radish flower data}

The data in this example are a subset of the data collected by \citet{ridley:ellstrand:2010}. The scientists were interested in whether non-native radishes had adapted to the climate they had  grown in for 150 years. In other words, they wanted to compare two types of radish to see whether each type would grow just as well in their own climate as they would in the other climate. \\

In this dataset, the response is the number of radish flowers. This is assumed to have a Poisson distribution.  \texttt{Site} is a categorical variable with two categories representing the two sites where plants were grown. The variable \texttt{Region} is categorical with two categories representing the two places in California from which the plants were taken. The variable \texttt{Pop} is a categorical variable representing the population of radish, and \texttt{Pop} is nested in \texttt{Region}. The variable \texttt{Block} is a categorical blocking variable nested in \texttt{Site}.  Following the example of \citet{ridley:ellstrand:2010}, \texttt{Block} and \texttt{Pop} are random while \texttt{Site} and \texttt{Region} are fixed. The scientists were interested in  the interaction between \texttt{Site} and \texttt{Region}, since that would indicate that radishes grow better in the area they have been grown during recent history.








\section[How to use glmm]{How to use \pkg{glmm}}\label{sec:howto}


\subsection{Formatting the data}

The following vectors can be used to fit a generalized linear mixed model using the \texttt{glmm} package. These vectors can be contained in a data frame, but they do not need to be.
\begin{enumerate}
\item The response vector(s). If your response is Poisson or Bernoulli, then you should have a single response vector. If your response is Poisson, then the entries in the response vector must be natural numbers. If your response is Bernoulli, then the entries in the response vector must be $0$ and $1$. If your response is binomial, then you should have two vectors connected with \texttt{cbind}: the first vector should be the number of successes and the second should be the number of failures.
\item At least one vector that will be used for defining the random effects' design matrix. For this version of \texttt{glmm}, the vector(s) should be class \texttt{factor}.
\item Vector(s) that will be used for defining the fixed effects' design matrix. The vector(s) can be of class \texttt{factor} or \texttt{numeric}. 
\end{enumerate}
The first two types of vectors described in the list are required. The last type is optional. That is, the minimum requirement to fit a \texttt{glmm} model is the response vector and one vector for defining the random effects' design matrix. \\









\begin{CodeChunk}
\begin{CodeInput}
input code
\end{CodeInput}
\begin{CodeOutput}
output code
\end{CodeOutput}
\begin{CodeInput}
second input
\end{CodeInput}
\end{CodeChunk}





\subsection{Using the model-fitting function}

  Following Model A from \citet{karim:zeger:1992}, we set \texttt{Mate} as the response, \texttt{Cross} as the fixed effect variable, and \texttt{Female} and \texttt{Male} as the random effect variables. That is, we would like to fit a generalized linear mixed model with a logit link (because the response is Bernoulli). We will have four fixed effect parameters ($\beta_{R/R}, \beta_{R/W},\beta_{W/R},\beta_{W/W}$). There is likely to be variability among the females and variability among the males. That is, some females will be more likely to mate than other females, and we would like the model to reflect the tendencies of the individual salamanders. We incorporate this into the model by including a random effect for each female salamander and a random effect for each male salamander. We believe the female salamanders' random effects are i.i.d. draws from $N(0, \nu_F)$, where $\nu_F$ is an unknown parameter to be estimated. Similarly, we believe the male salamanders' random effects are i.i.d. draws from $N(0,\nu_M)$, where $\nu_M$ is an unknown parameter to be estimated. Finally, we believe the female and male random effects are independent of one another. \\




In the following code, we  fit the model using the \texttt{glmm} command and save the model under the name  \texttt{sal}. Because \texttt{Mate} is our response, it is on the left of the $\sim$. We want to have a fixed effect for each of the four levels of \texttt{Cross}, so we type \texttt{Mate $\sim$ 0 + Cross}. Because \texttt{Cross} is a factor, typing \texttt{Mate $\sim$ Cross} would fit an equivalent model. \\ 

Next, the \texttt{random} list creates the design matrices for the random effects. Since we want two random effects for each cross (one from the female salamander and one from the male salamander), we type \texttt{list($\sim$ 0 + Female, $\sim$ 0 + Male)}. We  include the \texttt{0} because we want our random effects to be centered at 0. Almost always, you will want your random effects to have mean 0. \\

 Following the \texttt{random} list, the argument \texttt{varcomps.names} allows us to name the list of variance components. In the \texttt{random} list, we have placed the females first. Therefore, the order of the variance components names are first ``F'' and then ``M.''  \\

Next,  we specify the name of our data set. This is an optional argument. If the data set is not specified, \texttt{glmm} looks to the parent environment for the variables you have referenced. \\

After the name of the data set, we need to specify the type of the response. In the salamander mating example, the response is binary: the salamanders either mated or they did not. Therefore, the family is \texttt{bernoulli.glmm}. If your response is a count, then the family is \texttt{poisson.glmm}.\\

Next, we specify our Monte Carlo sample size \texttt{m}. The general rule is the larger the Monte Carlo sample size, the more accurate the Monte Carlo likelihood approximation (MCLA) will be, and the more accurate the resulting Monte Carlo maximum likelihood estimates (MCMLEs) will be. Ideally, you want the largest \texttt{m} that time allows. For this vignette, we have chosen a Monte Carlo sample size that allows for quick computation. If you are interested in accuracy in the resulting estimates for the salamander model, we suggest a larger Monte Carlo sample size.  \\

%Finally, we can decide whether we would like additional output (see details in section \ref{sec:otherstuff}). If we would like the additional output, we type \texttt{debug = TRUE}. The default is \texttt{debug = FALSE}.\\

We put this all together in the following commands. Note that we set the seed so that we can have reproducible results. In other words, if you set your seed to the same number and type the exact command listed below, your results should be identical to those listed here. Additionally, the \texttt{proc.time} commands have been used to give you an idea of how quickly the model can be fit. The times shown here are from fitting a model on an ultrabook that cost 500 USD in 2013. 


\subsection{Using the summary output}
The \texttt{summary} command displays
\begin{itemize}
\item the function call (to remind you of the model you fit).
\item the link function.
\item the fixed effect estimates, their standard errors (calculated using observed Fisher information), their \texttt{z value} test statistics (testing whether the coefficients are significantly different from zero), the test's p-values, and the R-standard significance stars (optional).
\item the variance component estimates, their standard errors (calculated using observed Fisher information), their \texttt{z value} test statistics (testing whether the coefficients are significantly different from zero), the test's p-values, and the R-standard significance stars (optional).
\end{itemize}



Note that the p-value for the fixed effects is calculated using a two-sided alternative hypothesis ($H_A: \beta \neq 0$) while the p-value for the variance components is calculated using a one-sided alternative hypothesis ($H_A: \nu >0$) because variance components must be nonnegative.\\


R stuff here

Looking at our output, we can see that the type of cross significantly affects the salamanders' odds of mating. Additionally, both the variance components are significantly different from zero and should be retained in the model.\\

The summary provides the estimates needed to write our model. First, we establish a little notation. Let $\pi_i$ represent the probability of successful mating for salamander pair $i$. Let $I()$ be an indicator function, so that $I\text{(Cross=R/R)}$ is $1$ when the variable \texttt{Cross = R/R} and 0 otherwise. Let $u_i^F$ represent the random effect from the female salamander in the $i$th pair. Let $u_i^M$ represent the random effect from the male salamander in the $i$th pair. Since the response is Bernoulli, the canonical link is the log odds of successful mating. Using this notation, we write the model as follows.\\
\begin{align*}
\log \left( \dfrac{\pi_i}{1-\pi_i} \right) &= 0.956 * I\text{(Cross=R/R)} + 0.2805 * I\text{(Cross=R/W)}\\
 &+ \ensuremath{-1.8968} * I\text{(Cross=W/R)}
+ 0.9723 * I\text{(Cross=W/W)}\\
 &+ u_i^F+u_i^M\\
u_i^F \overset{i.i.d.}{\sim} &N(0,1.288)\\
u_i^M \overset{i.i.d.}{\sim} &N(0,1.084)\\
\end{align*}

Recall that \texttt{m}  in the above model was chosen for convenience to save time. The resulting parameter estimates have a little too much variability. If we increase \texttt{m}, the Monte Carlo standard error decreases. 


\subsection{Calculating confidence intervals}

\subsection{Calculating Monte Carlo standard errors}

\subsection{Performing additional inference}


\section{Conclusion}

\bibliography{brref}



\end{document}
